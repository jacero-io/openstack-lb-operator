version: '3'

includes:
  utils:
    taskfile: utils.yaml
    internal: true

vars:
  CLUSTER_NAME: dev-cluster
  KIND_CONFIG_FILE: '{{.ROOT_DIR}}/hack/kind-config.yaml'
  HTTP_PORT: '8080'
  HTTPS_PORT: '8443'

tasks:
  create-kind-config:
    internal: true
    desc: Create a kind configuration file
    cmds:
      - |
        cat > {{.KIND_CONFIG_FILE}} << 'EOF'
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        name: {{.CLUSTER_NAME}}
        networking:
          # Set the API server address to bind to all interfaces
          apiServerAddress: "127.0.0.1"
          apiServerPort: 6443
          # Use a different pod subnet to avoid potential conflicts
          podSubnet: "10.244.0.0/16"
          # Use a different service subnet to avoid conflicts
          serviceSubnet: "10.96.0.0/16"
          # Explicitly disable disableDefaultCNI to ensure kindnet works
          disableDefaultCNI: false
        nodes:
        - role: control-plane
          extraPortMappings:
          - containerPort: 80
            hostPort: {{.HTTP_PORT}}
            protocol: TCP
          - containerPort: 443
            hostPort: {{.HTTPS_PORT}}
            protocol: TCP
        EOF
        echo "Created kind config at {{.KIND_CONFIG_FILE}}"

  create-cluster:
    desc: Create a new kind cluster
    cmds:
      - |
        if ! {{.KIND_CMD}} get clusters | grep -q "^{{.CLUSTER_NAME}}$"; then
          echo "Creating kind cluster {{.CLUSTER_NAME}}..."
          {{.KIND_CMD}} create cluster --name {{.CLUSTER_NAME}} --config {{.KIND_CONFIG_FILE}}
          mkdir -p {{.ROOT_DIR}}/hack
          {{.KIND_CMD}} export kubeconfig --name {{.CLUSTER_NAME}} --kubeconfig {{.KUBECONFIG}}
          {{.KIND_CMD}} export kubeconfig --name {{.CLUSTER_NAME}} --kubeconfig $HOME/.kube/config
        else
          echo "Cluster {{.CLUSTER_NAME}} already exists"
        fi
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    status:
      - '{{.KIND_CMD}} get clusters | grep -q "^{{.CLUSTER_NAME}}$"'
    deps:
      - create-kind-config
      - utils:kind

  install-components:
    desc: Install necessary components in the cluster
    vars:
      METALLB_VERSION: 0.14.9
    cmds:
      # Check API server connectivity first
      - |
        echo "Checking API server connectivity..."
        RETRY_COUNT=0
        MAX_RETRIES=10
        until {{.KUBECTL_CMD}} get nodes &>/dev/null || [ $RETRY_COUNT -eq $MAX_RETRIES ]; do
          echo "Waiting for API server to be accessible... (attempt $((RETRY_COUNT+1))/$MAX_RETRIES)"
          sleep 5
          RETRY_COUNT=$((RETRY_COUNT+1))
        done
        if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
          echo "Failed to connect to API server after $MAX_RETRIES attempts"
          exit 1
        fi
        echo "API server is accessible"
      
      # Install MetalLB using Helm
      - |
        echo "Installing MetalLB {{.METALLB_VERSION}}..."
        # Create namespace for MetalLB
        {{.KUBECTL_CMD}} create namespace metallb-system || true
        
        # Install MetalLB from Helm chart
        {{.HELM_CMD}} repo add metallb https://metallb.github.io/metallb
        {{.HELM_CMD}} repo update
        {{.HELM_CMD}} upgrade --install metallb metallb/metallb \
          --version {{.METALLB_VERSION}} \
          --namespace metallb-system \
          --set loadBalancerClass=metallb \
          --wait
        
        # Wait for MetalLB pods to be ready
        echo "Waiting for MetalLB to be ready..."
        {{.KUBECTL_CMD}} wait --namespace metallb-system \
          --for=condition=ready pod \
          --selector=app.kubernetes.io/name=metallb \
          --timeout=120s || true
        
        echo "MetalLB installed successfully. Use 'task devstack:configure-metallb' to configure it with DevStack network"
        
      # Install Envoy Gateway using Helm
      # - |
      #   echo "Installing Envoy Gateway..."
      #   # Create namespace for Envoy Gateway
      #   {{.KUBECTL_CMD}} create namespace envoy-gateway-system || true
      #   # Install Envoy Gateway from OCI registry
      #   {{.LOCALBIN}}/helm upgrade --install envoy-gateway oci://docker.io/envoyproxy/gateway-helm \
      #     --namespace envoy-gateway-system \
      #     --set "service.type=NodePort" \
      #     --wait
      #   echo "Waiting for Envoy Gateway to be ready..."
      #   {{.KUBECTL_CMD}} wait --namespace envoy-gateway-system \
      #     --for=condition=ready pod \
      #     --selector=app.kubernetes.io/name=envoy-gateway \
      #     --timeout=120s || true
      
      # Install Metrics Server
      # - |
      #   echo "Installing Metrics Server..."
      #   {{.KUBECTL_CMD}} apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
      #   {{.KUBECTL_CMD}} patch -n kube-system deployment metrics-server --type=json \
      #     -p '[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'
      
      # Install cert-manager (for using Kubebuilder with webhooks)
      # - |
      #   echo "Installing cert-manager..."
      #   # Create and apply cert-manager manifest
      #   {{.KUBECTL_CMD}} apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.14.3/cert-manager.yaml
      #   # Wait for the CRDs to be ready first
      #   echo "Waiting for cert-manager CRDs to be established..."
      #   for CRD in $({{.KUBECTL_CMD}} get crd -l app.kubernetes.io/name=cert-manager -o name); do
      #     {{.KUBECTL_CMD}} wait --for=condition=established --timeout=60s $CRD || true
      #   done
      #   # Then wait for the pods
      #   echo "Waiting for cert-manager pods to be ready..."
      #   {{.KUBECTL_CMD}} -n cert-manager wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager --timeout=120s || true
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    deps:
      - create-cluster
      - utils:kubectl
      - utils:helm

  start-cluster:
    desc: Start an existing kind cluster
    cmds:
      - |
        if ! {{.KIND_CMD}} get clusters | grep -q "^{{.CLUSTER_NAME}}$"; then
          echo "Cluster {{.CLUSTER_NAME}} does not exist. Run 'task kind:create-cluster' first."
          exit 1
        fi
        if ! docker ps | grep -q "{{.CLUSTER_NAME}}-control-plane"; then
          echo "Starting kind cluster {{.CLUSTER_NAME}}..."
          {{.KIND_CMD}} export kubeconfig --name {{.CLUSTER_NAME}} --kubeconfig {{.KUBECONFIG}}
          echo "Cluster started successfully"
        else
          echo "Cluster {{.CLUSTER_NAME}} is already running"
        fi
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    deps:
      - utils:kind

  stop-cluster:
    desc: Stop the kind cluster without deleting it
    cmds:
      - |
        if {{.KIND_CMD}} get clusters | grep -q "^{{.CLUSTER_NAME}}$"; then
          echo "Stopping kind cluster {{.CLUSTER_NAME}}..."
          # Find and stop the Docker containers for the cluster
          CONTAINERS=$(docker ps -q --filter=name="{{.CLUSTER_NAME}}")
          if [ -n "$CONTAINERS" ]; then
            docker stop $CONTAINERS
            echo "Cluster stopped successfully"
          else
            echo "No running containers found for cluster {{.CLUSTER_NAME}}"
          fi
        else
          echo "Cluster {{.CLUSTER_NAME}} does not exist"
        fi
    deps:
      - utils:kind

  cleanup-cluster:
    desc: Delete the kind cluster and clean up resources
    cmds:
      - |
        if {{.KIND_CMD}} get clusters | grep -q "^{{.CLUSTER_NAME}}$"; then
          echo "Deleting kind cluster {{.CLUSTER_NAME}}..."
          {{.KIND_CMD}} delete cluster --name {{.CLUSTER_NAME}}
          echo "Cluster deleted successfully"
        else
          echo "Cluster {{.CLUSTER_NAME}} does not exist"
        fi
      # Optionally remove config file
      - |
        if [ -f {{.KIND_CONFIG_FILE}} ]; then
          rm {{.KIND_CONFIG_FILE}}
          echo "Removed configuration file"
        fi
      # Clean up any stale Docker networks
      - |
        echo "Cleaning up any stale Docker networks..."
        STALE_NETWORKS=$(docker network ls --filter label=io.k8s.sigs.kind.cluster --format '{{.Name}}' | grep kind || true)
        if [ -n "$STALE_NETWORKS" ]; then
          for NETWORK in $STALE_NETWORKS; do
            if ! docker network inspect $NETWORK | grep -q '"Containers": {}'; then
              echo "Removing containers from network $NETWORK"
              CONTAINERS=$(docker network inspect $NETWORK -f '{{range $k, $v := .Containers}}{{$k}} {{end}}')
              for CONTAINER in $CONTAINERS; do
                docker network disconnect -f $NETWORK $CONTAINER || true
              done
            fi
            docker network rm $NETWORK || true
          done
        fi
    deps:
      - utils:kind

  status:
    desc: Check the status of the kind cluster
    cmds:
      - |
        if {{.KIND_CMD}} get clusters | grep -q "^{{.CLUSTER_NAME}}$"; then
          echo "Cluster {{.CLUSTER_NAME}} exists"
          if docker ps | grep -q "{{.CLUSTER_NAME}}-control-plane"; then
            echo "Cluster status: Running"
            echo "Nodes:"
            {{.KUBECTL_CMD}} get nodes
            echo "\nPods in kube-system:"
            {{.KUBECTL_CMD}} get pods -n kube-system
          else
            echo "Cluster status: Stopped"
          fi
        else
          echo "Cluster {{.CLUSTER_NAME}} does not exist"
        fi
    env:
      KUBECONFIG: '{{.KUBECONFIG}}'
    deps:
      - utils:kind
      - utils:kubectl
